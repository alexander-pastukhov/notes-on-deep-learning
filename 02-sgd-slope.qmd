# Stochastic Gradient Descent for slope


## Generate data distributed normally relative to the slope

```{r}
#| warning: false
#| message: false
library(tidyverse)

set.seed(128230565)
x_range <- c(-15, 15)
slope <- 3
slope_df <- 
  tibble(x = x_range[1]:x_range[2]) %>%
  mutate(y = rnorm(n(), mean = x * slope, sd = 10))
```

Here is the dependence visualized with the blue line representing a true slope and the red line --- the sample slope.
```{r}
#| echo: false
sample_slope <- coef(lm(y ~ x - 1, slope_df))[1]

ggplot(slope_df, aes(x = x, y = y)) +
  geom_point() +
  geom_abline(slope = slope, color = "blue", intercept = 0) +
  geom_abline(slope = sample_slope, color = "red", intercept = 0)
```

## Direction of adjusment based on difference and input

```{r}
#| echo: false
library(patchwork)

conditions <- c("Slope too high\nNegative input (-) * positive delta (+)\nReduce weight (-)",
                "Slope too high\nPositive input (+) * negative delta (-)\nReduce weight (-)",
                "Slope too low\nPositive input (+) * positive delta (+)\nIncrease weight (+)",
               "Slope too low\nNegative input (-) * negative delta (-)\nIncrease weight (+)")

example_slope <- tibble(Slope = sample_slope + 10 * c(1, 1, -1, -1),
                        Condition = conditions) 

example_slope_lines <-
  example_slope %>%
  group_by(Condition) %>%
  summarise(x = x_range,
            y = Slope[1] * x,
            .groups = "drop")

example_point <-
  tibble(Condition = conditions,
         Curvature = 0.5 * c(-1, -1, 1, 1),
         x = slope_df[['x']][c(5, 25, 25, 5)],
         y = slope_df[['x']][c(5, 25, 25, 5)]) %>%
  mutate(ypred = x * example_slope[['Slope']])

example_plots <- list()
for(iC in 1:length(conditions)){
  example_plots[[iC]] <-
    ggplot(data = filter(example_slope_lines, Condition == conditions[iC]), aes(x = x, y = y)) +
    geom_line(color = "orange") +
    geom_point(data = slope_df, aes(x = x, y = y)) +
    geom_point(data = filter(example_point, Condition == conditions[iC]), color = "red") +
    geom_curve(data = filter(example_point, Condition == conditions[iC]), aes(x = x, y = ypred, xend = x, yend = y),
               curvature = filter(example_point, Condition == conditions[iC])[['Curvature']][1], arrow = arrow(length = unit(0.03, "npc"))) +
    labs(subtitle = conditions[iC])
}

(example_plots[[1]] + example_plots[[2]])  / (example_plots[[3]] + example_plots[[4]])
```

Thus, the solution is to multiply the delta by input, which also serves to modulate it. The logic is that since the error depends on the product of weight and inputs, higher input means that it contributes more to the error and so must be corrected more aggressively, whereas a small input contributes little and its adjustment should be equally small.

## Weight delta logic via derivatives

The logic of computing the adjustment above is an illustrative way of understanding how a change in weight would propagate into a change in error. In other words, we are interested if we change weight by a small amount $dw$ what would be the change in error $dL_2$. This is a definition of a _derivative_ and is written as $dL_2/dw$. On the one hand, computing this derivative is not particularly easy, as our error is a function of a loss $L2()$ which is a function of activation $L2(\sigma())$ which is a function of a linear weighted sum function $L2(\sigma(f_{wsum}(x, w)))$.

On the other hand, nesting is a simple case because we can use a chain rule. If you have $f1(f2(x))$ then 

$$\frac{df1}{dx} = \frac{df1}{df2} \frac{df2}{dx}$$

In our case 
$$L2 = (y - \sigma(wx))^2$$

$$ L_2 = ()^2 ← \sigma() ← wx $$

$$\frac{d}{dx} x^n = n x^{n-1}$$

$$\frac{d}{dx} x^n = n x^{n-1}$$

## SDG step by step

First, we make a random guess about the slope.
```{r}
# generate a random initial guess about the mean
set.seed(8739172)
current_guess_at_slope <- rnorm(1, sd = 10)
```

It is visualized as an orange line, whereas the sample mean (not the true sampling distribution mean!) is black.
```{r}
#| echo: false
ggplot(slope_df, aes(x = x, y = y)) +
  geom_point() +
  geom_abline(slope = sample_slope, color = "black", intercept = 0) +
  geom_abline(slope = current_guess_at_slope, color = "orange", intercept = 0)
```

Next we pick a first (or, it could be random) data point and compute difference between it and our guess about the mean. Note that delta is only a part of derivative. Our formula for quadratic loss is 
$$L2 = (wx - y)^2 = w^2x^2 - 2wxy+y^2$$

the derivative with respect to the weight is 

$$\frac{dL2}{dx} = 2x^2w - 2xy = 2x (wx - y)$$ 

```{r}
# pick a random data point
ipick <- 1

# computing squared error and difference
prediction <- current_guess_at_slope * slope_df[['x']][ipick]
error <- (prediction - slope_df[['y']][ipick])^2
delta <- prediction - slope_df[['y']][ipick]
```

The difference and the direction in which we need to adjust our guess are depicted as the arrow.
```{r}
#| echo: false
dot_curvature <- - 0.1 * sign(delta * slope_df[['x']][ipick])
ggplot(slope_df, aes(x = x, y = y)) +
  geom_abline(slope = sample_slope, color = "black", intercept = 0) +
  geom_abline(slope = current_guess_at_slope, color = "orange", intercept = 0) +
  geom_point() +
  geom_point(aes(x = .data[['x']][ipick], y = .data[['y']][ipick]), color = "red") +
  geom_curve(aes(x = .data[['x']][ipick], y = prediction, xend = .data[['x']][ipick], yend = .data[['y']][ipick]),
               curvature = 0.1, arrow = arrow(length = unit(0.03, "npc"))) 
```

```{r}
#| echo: false
deriv_slope <- 2 * slope_df[['x']][ipick] * (current_guess_at_slope * slope_df[['x']][ipick] - slope_df[['y']][ipick])
deriv_intercept <-  (slope_df[['y']][ipick] - current_guess_at_slope * slope_df[['x']][ipick])^2 - deriv_slope * current_guess_at_slope

tibble(Slope = seq(-20, 20, length.out = 100),
       Error = ( Slope * slope_df[['x']][ipick] - slope_df[['y']][ipick])^2) %>%
  ggplot(aes(x = Slope, y = Error)) +
  geom_line() + 
  geom_point(x = current_guess_at_slope, y = (slope_df[['y']][ipick] - current_guess_at_slope * slope_df[['x']][ipick])^2, color = "orange") +
  geom_abline(intercept = deriv_intercept,
              slope = deriv_slope)
```

Finally, we move our guess by a small amount to minimize the error.
```{r}
#| echo: false
# we save this behind the scenes only for comparison plotting 
previous_guess <- current_guess_at_slope
```
```{r}
alpha <- 0.001
current_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]
```

Here, the old guess is marked by a dashed line. Our step is probably too big but here I made `alpha` largish to make seeing the difference easier.
```{r}
#| echo: false
ggplot(slope_df, aes(x = x, y = y)) +
  geom_abline(slope = sample_slope, color = "black", intercept = 0) +
  geom_abline(slope = current_guess_at_slope, color = "orange", intercept = 0) +
  geom_abline(slope = previous_guess, color = "orange", intercept = 0, linetype = "dashed") +
  geom_point() +
  geom_point(aes(x = .data[['x']][ipick], y = .data[['y']][ipick]), color = "red") 
```

## SDG over all points and many epochs
```{r}
#| eval: false

# learning rate
alpha <- 0.0001

# generate a random initial guess about the mean
set.seed(8739172)
current_guess_at_slope <- rnorm(1, sd = 10)

for(epoch in 1:30){
  total_error <- 0
  for(ipick in 1:nrow(slope_df)){

    # prediction from our neural network model
    prediction <- current_guess_at_slope * slope_df[['x']][ipick]

    # computing squared error and difference
    error <- (prediction - slope_df[['y']][ipick])^2
    total_error <- total_error + error
    delta <- prediction - slope_df[['y']][ipick]
    
    # adjusting our guess
    current_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]
  }
  # computing and reporting error
  cat(glue::glue("Epoch: {epoch}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_slope, 2)}. Sample slope: {round(sample_slope, 2)}"), "\n")
}
```

```{r}
#| eval: false
#| echo: false
#| message: false
#| warning: false
library(av)

generate_sdg_plots <- function(epochs_n = 1){
  # learning rate
alpha <- 0.0001

# generate a random initial guess about the mean
set.seed(8739172)
current_guess_at_slope <- rnorm(1, sd = 10)

for(epoch in 1:epochs_n){
  total_error <- 0
  for(ipick in 1:nrow(slope_df)){
      # computing squared error and difference
      prediction <- current_guess_at_slope * slope_df[['x']][ipick]
      error <- (prediction - slope_df[['y']][ipick])^2
      delta <- prediction - slope_df[['y']][ipick]
      
      total_prediction <- current_guess_at_slope * slope_df[['x']]
      total_error <- sum((total_prediction - slope_df[['y']])^2)
      
      # adjusting our guess
      current_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]
      
      dot_curvature <- - 0.1 * sign(delta * slope_df[['x']][ipick])
      slope_plot <- 
        ggplot(slope_df, aes(x = x, y = y)) +
        geom_abline(slope = sample_slope, color = "black", intercept = 0) +
        geom_abline(slope = current_guess_at_slope, color = "orange", intercept = 0) +
        geom_point() +
        geom_point(aes(x = .data[['x']][ipick], y = .data[['y']][ipick]), color = "red") +
        geom_curve(aes(x = .data[['x']][ipick], y = prediction, xend = .data[['x']][ipick], yend = .data[['y']][ipick]),
                     curvature = dot_curvature, arrow = arrow(length = unit(0.03, "npc"))) +
        labs(subtitle = glue::glue("Epoch/Point: {epoch}/{ipick}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_slope, 2)}. Sample mean: {round(sample_slope, 2)}"))
      
      print(slope_plot)
    }
  }
}

video_file <- fs::path('videos', 'finding-slope-via-sgd.mp4')
av::av_capture_graphics(generate_sdg_plots(30), video_file, 1280, 720, res = 144, framerate=15)
```

{{< video videos/finding-slope-via-sgd.mp4 >}}

