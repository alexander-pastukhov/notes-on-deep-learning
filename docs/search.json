[
  {
    "objectID": "01-sgd-mean.html",
    "href": "01-sgd-mean.html",
    "title": "",
    "section": "",
    "text": "As a first example, we will use stochastic gradient descent (SDG) to estimate the mean of a set of values.\n\n\n\nlibrary(tidyverse)\n\nset.seed(128230565)\nn <- 30\nnormal_df <- tibble(`data point index` = 1:n,\n                    y = rnorm(n, mean = 0, sd = 1))\n\nmeans_df <- tibble(the_mean = c(0, mean(normal_df$y)),\n                   \"Kind of mean\" = c(\"True mean\", \"Sample mean\"))\n\n\n\n\n\n\nTo visualize our stochastic gradient descent, it is better to plot each data point as a point and our means as horizontal lines. Note, that this is just a different representation and x-axis has no meaning beyond spreading the dots horizontally.\n\n\n\n\n\n\n\n\nFirst, we make a random guess about the mean.\n\n# generate a random initial guess about the mean\nset.seed(873917234)\ncurrent_guess_at_mean <- rnorm(1, sd = 3)\n\nIt is visualized as an orange line, whereas the sample mean (not the true sampling distribution mean!) is black.\n\n\n\n\n\nNext we pick a first (or, it could be random) data point and compute difference between it and our guess about the mean.\n\n# pick a random data point\nipick <- 1\n\n# computing squared error and difference\nerror <- (current_guess_at_mean - normal_df[['y']][ipick])^2\ndelta <- current_guess_at_mean - normal_df[['y']][ipick]\n\nThe difference and the direction in which we need to adjust our guess are depicted as the arrow.\n\n\n\n\n\nFinally, we move our guess by a small amount to minimize the error\n\n\n\n\nalpha <- 0.05\ncurrent_guess_at_mean <- current_guess_at_mean - alpha * delta\n\nHere, the old guess is marked by a dashed line. Our step is probably too big but here I made alpha largish to make seeing the difference easier.\n\n\n\n\n\n\n\n\n\n# learning rate\nalpha <- 0.005\n\n# generate a random initial guess about the mean\nset.seed(873917234)\ncurrent_guess_at_mean <- rnorm(1, sd = 3)\n\nfor(epoch in 1:50){\n  total_error <- 0\n  for(ipick in 1:nrow(normal_df)){\n    # computing squared error and difference\n    error <- (current_guess_at_mean - normal_df[['y']][ipick])^2\n    total_error <- total_error + error\n    delta <- current_guess_at_mean - normal_df[['y']][ipick]\n    \n    # adjusting our guess\n    current_guess_at_mean <- current_guess_at_mean - alpha * delta\n  }\n  # computing and reporting error\n  cat(\"\\r\", glue::glue(\"Epoch: {epoch}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_mean, 2)}. Sample mean: {round(mean(normal_df[['y']]), 2)}\"))\n}"
  },
  {
    "objectID": "02-sgd-slope.html",
    "href": "02-sgd-slope.html",
    "title": "",
    "section": "",
    "text": "library(tidyverse)\n\nset.seed(128230565)\nx_range <- c(-15, 15)\nslope <- 3\nslope_df <- \n  tibble(x = x_range[1]:x_range[2]) %>%\n  mutate(y = rnorm(n(), mean = x * slope, sd = 10))\n\nHere is the dependence visualized with the blue line representing a true slope and the red line â€” the sample slope.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, we make a random guess about the slope.\n\n# generate a random initial guess about the mean\nset.seed(8739172)\ncurrent_guess_at_slope <- rnorm(1, sd = 10)\n\nIt is visualized as an orange line, whereas the sample mean (not the true sampling distribution mean!) is black.\n\n\n\n\n\nNext we pick a first (or, it could be random) data point and compute difference between it and our guess about the mean.\n\n# pick a random data point\nipick <- 1\n\n# computing squared error and difference\nprediction <- current_guess_at_slope * slope_df[['x']][ipick]\nerror <- (prediction - slope_df[['y']][ipick])^2\ndelta <- prediction - slope_df[['y']][ipick]\n\nThe difference and the direction in which we need to adjust our guess are depicted as the arrow.\n\n\n\n\n\nFinally, we move our guess by a small amount to minimize the error.\n\n\n\n\nalpha <- 0.001\ncurrent_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]\n\nHere, the old guess is marked by a dashed line. Our step is probably too big but here I made alpha largish to make seeing the difference easier.\n\n\n\n\n\n\n\n\n\n# learning rate\nalpha <- 0.0001\n\n# generate a random initial guess about the mean\nset.seed(8739172)\ncurrent_guess_at_slope <- rnorm(1, sd = 10)\n\nfor(epoch in 1:30){\n  total_error <- 0\n  for(ipick in 1:nrow(slope_df)){\n    \n    # computing squared error and difference\n    prediction <- current_guess_at_slope * slope_df[['x']][ipick]\n    error <- (prediction - slope_df[['y']][ipick])^2\n    total_error <- total_error + error\n    delta <- prediction - slope_df[['y']][ipick]\n    \n    # adjusting our guess\n    current_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]\n  }\n  # computing and reporting error\n  cat(glue::glue(\"Epoch: {epoch}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_slope, 2)}. Sample slope: {round(sample_slope, 2)}\"), \"\\n\")\n}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Preface\nThese are companion notes for a deep learning seminar."
  }
]