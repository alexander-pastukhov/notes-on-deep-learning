[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on Deep Learning",
    "section": "",
    "text": "These are companion notes for a deep learning seminar."
  },
  {
    "objectID": "01-sgd-mean.html#generate-normally-distributed-data",
    "href": "01-sgd-mean.html#generate-normally-distributed-data",
    "title": "2  Stochastic Gradient Descent for a mean",
    "section": "2.1 Generate normally distributed data",
    "text": "2.1 Generate normally distributed data\n\nlibrary(tidyverse)\n\nset.seed(128230565)\nn <- 30\nnormal_df <- tibble(`data point index` = 1:n,\n                    y = rnorm(n, mean = 0, sd = 1))\n\nmeans_df <- tibble(the_mean = c(0, mean(normal_df$y)),\n                   \"Kind of mean\" = c(\"True mean\", \"Sample mean\"))\n\n\n\n\n\n\nTo visualize our stochastic gradient descent, it is better to plot each data point as a point and our means as horizontal lines. Note, that this is just a different representation and x-axis has no meaning beyond spreading the dots horizontally."
  },
  {
    "objectID": "01-sgd-mean.html#sdg-step-by-step",
    "href": "01-sgd-mean.html#sdg-step-by-step",
    "title": "2  Stochastic Gradient Descent for a mean",
    "section": "2.2 SDG step by step",
    "text": "2.2 SDG step by step\nFirst, we make a random guess about the mean.\n\n# generate a random initial guess about the mean\nset.seed(873917234)\ncurrent_guess_at_mean <- rnorm(1, sd = 3)\n\nIt is visualized as an orange line, whereas the sample mean (not the true sampling distribution mean!) is black.\n\n\n\n\n\nNext we pick a first (or, it could be random) data point and compute difference between it and our guess about the mean.\n\n# pick a random data point\nipick <- 1\n\n# computing squared error and difference\nerror <- (current_guess_at_mean - normal_df[['y']][ipick])^2\ndelta <- current_guess_at_mean - normal_df[['y']][ipick]\n\nThe difference and the direction in which we need to adjust our guess are depicted as the arrow.\n\n\n\n\n\nFinally, we move our guess by a small amount to minimize the error\n\n\n\n\nalpha <- 0.05\ncurrent_guess_at_mean <- current_guess_at_mean - alpha * delta\n\nHere, the old guess is marked by a dashed line. Our step is probably too big but here I made alpha largish to make seeing the difference easier."
  },
  {
    "objectID": "01-sgd-mean.html#sdg-over-all-points-and-many-epochs",
    "href": "01-sgd-mean.html#sdg-over-all-points-and-many-epochs",
    "title": "2  Stochastic Gradient Descent for a mean",
    "section": "2.3 SDG over all points and many epochs",
    "text": "2.3 SDG over all points and many epochs\n\n# learning rate\nalpha <- 0.005\n\n# generate a random initial guess about the mean\nset.seed(873917234)\ncurrent_guess_at_mean <- rnorm(1, sd = 3)\n\nfor(epoch in 1:50){\n  total_error <- 0\n  for(ipick in 1:nrow(normal_df)){\n    # computing squared error and difference\n    error <- (current_guess_at_mean - normal_df[['y']][ipick])^2\n    total_error <- total_error + error\n    delta <- current_guess_at_mean - normal_df[['y']][ipick]\n    \n    # adjusting our guess\n    current_guess_at_mean <- current_guess_at_mean - alpha * delta\n  }\n  # computing and reporting error\n  cat(\"\\r\", glue::glue(\"Epoch: {epoch}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_mean, 2)}. Sample mean: {round(mean(normal_df[['y']]), 2)}\"))\n}"
  },
  {
    "objectID": "02-sgd-slope.html#generate-data-distributed-normally-relative-to-the-slope",
    "href": "02-sgd-slope.html#generate-data-distributed-normally-relative-to-the-slope",
    "title": "3  Stochastic Gradient Descent for slope",
    "section": "3.1 Generate data distributed normally relative to the slope",
    "text": "3.1 Generate data distributed normally relative to the slope\n\nlibrary(tidyverse)\n\nset.seed(128230565)\nx_range <- c(-15, 15)\nslope <- 3\nslope_df <- \n  tibble(x = x_range[1]:x_range[2]) %>%\n  mutate(y = rnorm(n(), mean = x * slope, sd = 10))\n\nHere is the dependence visualized with the blue line representing a true slope and the red line — the sample slope."
  },
  {
    "objectID": "02-sgd-slope.html#direction-of-adjusment-based-on-difference-and-input",
    "href": "02-sgd-slope.html#direction-of-adjusment-based-on-difference-and-input",
    "title": "3  Stochastic Gradient Descent for slope",
    "section": "3.2 Direction of adjusment based on difference and input",
    "text": "3.2 Direction of adjusment based on difference and input"
  },
  {
    "objectID": "02-sgd-slope.html#sdg-step-by-step",
    "href": "02-sgd-slope.html#sdg-step-by-step",
    "title": "3  Stochastic Gradient Descent for slope",
    "section": "3.3 SDG step by step",
    "text": "3.3 SDG step by step\nFirst, we make a random guess about the slope.\n\n# generate a random initial guess about the mean\nset.seed(8739172)\ncurrent_guess_at_slope <- rnorm(1, sd = 10)\n\nIt is visualized as an orange line, whereas the sample mean (not the true sampling distribution mean!) is black.\n\n\n\n\n\nNext we pick a first (or, it could be random) data point and compute difference between it and our guess about the mean.\n\n# pick a random data point\nipick <- 1\n\n# computing squared error and difference\nprediction <- current_guess_at_slope * slope_df[['x']][ipick]\nerror <- (prediction - slope_df[['y']][ipick])^2\ndelta <- prediction - slope_df[['y']][ipick]\n\nThe difference and the direction in which we need to adjust our guess are depicted as the arrow.\n\n\n\n\n\nFinally, we move our guess by a small amount to minimize the error.\n\n\n\n\nalpha <- 0.001\ncurrent_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]\n\nHere, the old guess is marked by a dashed line. Our step is probably too big but here I made alpha largish to make seeing the difference easier."
  },
  {
    "objectID": "02-sgd-slope.html#sdg-over-all-points-and-many-epochs",
    "href": "02-sgd-slope.html#sdg-over-all-points-and-many-epochs",
    "title": "3  Stochastic Gradient Descent for slope",
    "section": "3.4 SDG over all points and many epochs",
    "text": "3.4 SDG over all points and many epochs\n\n# learning rate\nalpha <- 0.0001\n\n# generate a random initial guess about the mean\nset.seed(8739172)\ncurrent_guess_at_slope <- rnorm(1, sd = 10)\n\nfor(epoch in 1:30){\n  total_error <- 0\n  for(ipick in 1:nrow(slope_df)){\n\n    # prediction from our neural network model\n    prediction <- current_guess_at_slope * slope_df[['x']][ipick]\n\n    # computing squared error and difference\n    error <- (prediction - slope_df[['y']][ipick])^2\n    total_error <- total_error + error\n    delta <- prediction - slope_df[['y']][ipick]\n    \n    # adjusting our guess\n    current_guess_at_slope <- current_guess_at_slope - alpha * delta * slope_df[['x']][ipick]\n  }\n  # computing and reporting error\n  cat(glue::glue(\"Epoch: {epoch}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_slope, 2)}. Sample slope: {round(sample_slope, 2)}\"), \"\\n\")\n}"
  },
  {
    "objectID": "03-sgd-minibatch.html#generate-data-distributed-normally-relative-to-the-slope",
    "href": "03-sgd-minibatch.html#generate-data-distributed-normally-relative-to-the-slope",
    "title": "4  Visualizing Minibatch Stochastic Gradient Descent for slope",
    "section": "4.1 Generate data distributed normally relative to the slope",
    "text": "4.1 Generate data distributed normally relative to the slope\n\nlibrary(tidyverse)\n\nset.seed(128230565)\nx_range <- c(-15, 14)\nslope <- 3\nslope_df <-\n  tibble(x = x_range[1]:x_range[2]) %>%\n  mutate(y = rnorm(n(), mean = x * slope, sd = 10))\n\nHere is the dependence visualized with the blue line representing a true slope and the red line — the sample slope."
  },
  {
    "objectID": "03-sgd-minibatch.html#sdg-step-by-step",
    "href": "03-sgd-minibatch.html#sdg-step-by-step",
    "title": "4  Visualizing Minibatch Stochastic Gradient Descent for slope",
    "section": "4.2 SDG step by step",
    "text": "4.2 SDG step by step\nFirst, we make a random guess about the slope.\n\n# generate a random initial guess about the mean\nset.seed(8739172)\ncurrent_guess_at_slope <- rnorm(1, sd = 10)\n\nIt is visualized as an orange line, whereas the sample mean (not the true sampling distribution mean!) is black.\n\n\n\n\n\nNext we pick a first batch of 5 points and compute difference between it and our guess about the mean.\n\n# pick a random data point\nbatch_size <- 5\nipick <- 1:5\n\n# that's our neural network\nprediction <- current_guess_at_slope * slope_df[['x']][ipick] \n\n# computing squared error and difference\nerror <- (prediction - slope_df[['y']][ipick])^2\ndelta <- prediction - slope_df[['y']][ipick]\n\nThe difference and the direction in which we need to adjust our guess are depicted as the arrow.\n\n\n\n\n\nFinally, we move our guess by a small amount to minimize the error.\n\n\n\n\nalpha <- 0.001 / batch_size # we need a smaller step as it reflects delta * input accumulated over batch_size points\ncurrent_guess_at_slope <- current_guess_at_slope - alpha * sum(delta * slope_df[['x']][ipick])\n\nHere, the old guess is marked by a dashed line. Our step is probably too big but here I made alpha largish to make seeing the difference easier."
  },
  {
    "objectID": "03-sgd-minibatch.html#sdg-over-all-points-and-many-epochs",
    "href": "03-sgd-minibatch.html#sdg-over-all-points-and-many-epochs",
    "title": "4  Visualizing Minibatch Stochastic Gradient Descent for slope",
    "section": "4.3 SDG over all points and many epochs",
    "text": "4.3 SDG over all points and many epochs\n\n# mini batch size\nbatch_size <- 5\nbatches_n <- nrow(slope_df) / batch_size\n\n# learning rate\nalpha <- 0.0001  # we need a smaller step as it reflects delta * input accumulated over batch_size points\n\n# generate a random initial guess about the mean\nset.seed(8739172)\ncurrent_guess_at_slope <- rnorm(1, sd = 10)\n\nfor(epoch in 1:30){\n  total_error <- 0\n  for(ibatch in 1:batches_n){\n    ipick <- ((ibatch - 1) * batch_size + 1) : (ibatch * batch_size)\n  \n    # computing squared error and difference\n    prediction <- current_guess_at_slope * slope_df[['x']][ipick]\n    error <- (prediction - slope_df[['y']][ipick])^2\n    total_error <- total_error + sum(error)\n    delta <- prediction - slope_df[['y']][ipick]\n    \n    # adjusting our guess\n    current_guess_at_slope <- current_guess_at_slope - alpha * sum(delta * slope_df[['x']][ipick])\n  }\n  # computing and reporting error\n  cat(glue::glue(\"Epoch: {epoch}. Error: {round(total_error, 1)}. Estimate: {round(current_guess_at_slope, 2)}. Sample slope: {round(sample_slope, 2)}\"), \"\\n\")\n}"
  }
]